{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Problème - Session n°2 : une variable cachée"
      ],
      "metadata": {
        "id": "GYE9s4-HA1o_"
      },
      "id": "GYE9s4-HA1o_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce problème, on travaille sur un jeu de données comportant 50.000 entrées $x_i$ et des cibles $y_i$. Les entrées sont des vecteurs de taille 10 (au format torch), les cibles sont des scalaires construits à partir de cinq fonctions différentes ($f_0$, ..., $f_4$) : \\\n",
        "\n",
        "$$ \\forall i, \\exists k\\in [\\![0 \\;;4]\\!]  \\:\\: \\text{tel que} \\: f_k(x_i) = y_i $$\n",
        "\n",
        "Ces fonctions sont inconnues, ainsi que l'indice $k$. Par contre, on sait que le groupe des 1000 premières cibles ont été construites à partir du même indice  $k$, de même pour les mille  suivantes, et ainsi de suite.\n",
        "\n",
        "Le but est de parvenir à rassembler les groupes de cibles qui ont été générées avec le même indice $k$ (avec la même fonction)."
      ],
      "metadata": {
        "id": "pknhArHLLPP-"
      },
      "id": "pknhArHLLPP-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example d'échantillonnage du dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "! git clone https://github.com/Zakaria-Yahya/exam_2025_session2.git\n",
        "! cp exam_2025_session2/utils/utils.py .\n",
        "from utils import Problem1Dataset\n",
        "\n",
        "dataset = Problem1Dataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    x_batch, y_batch, k_batch, idx_batch = batch\n",
        "    print(\"Batch input shape:\", x_batch.shape)\n",
        "    print(\"Batch target shape:\", y_batch.shape)\n",
        "    print(\"Batch k shape:\", k_batch.shape) # indice k (pas utilisable à l'entraînement)\n",
        "    print(\"Batch indices shape:\", idx_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "zqFIZKgTPK14",
        "outputId": "81c44801-fdb3-45e6-8e6e-4875ca49c064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zqFIZKgTPK14",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025_session2'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 93 (delta 28), reused 8 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (93/93), 729.62 KiB | 2.35 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Batch input shape: torch.Size([32, 10])\n",
            "Batch target shape: torch.Size([32, 1])\n",
            "Batch k shape: torch.Size([32])\n",
            "Batch indices shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consignes :**\n",
        "- Entraîner l'architecture proposée dans la cellule suivante.\n",
        "- Montrer que les vecteurs 2D de self.theta permettent de répondre\n",
        "  au problème posé.\n",
        "- Décrire le rôle de self.theta, du vector noise \\\n",
        " et ainsi que la raison de la division par 1000 (**indices // 1000** dans le code)."
      ],
      "metadata": {
        "id": "-q1zuebPPvob"
      },
      "id": "-q1zuebPPvob"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
        "        super(DeepMLP, self).__init__()\n",
        "        self.theta = nn.Parameter(torch.randn(50, 2))\n",
        "        self.fc1 = nn.Linear(input_dim + 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, indices):\n",
        "        theta_batch = self.theta[indices // 1000, :]\n",
        "        noise = torch.normal(mean=torch.zeros_like(theta_batch),\n",
        "                             std=torch.ones_like(theta_batch))\n",
        "        x = torch.cat([x, theta_batch + noise], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x, theta_batch"
      ],
      "metadata": {
        "id": "idbO80HfPdiQ"
      },
      "id": "idbO80HfPdiQ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from utils import Problem1Dataset\n",
        "\n",
        "\n",
        "\n",
        "# Chargement du jeu de données\n",
        "jeu_donnees = Problem1Dataset()\n",
        "chargeur_donnees = DataLoader(jeu_donnees, batch_size=128, shuffle=True)  # Augmentation de la taille du lot pour un entraînement plus rapide\n",
        "\n",
        "# Vérification de la disponibilité du GPU et définition du périphérique\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device utilisé : {device}\")\n",
        "\n",
        "# Initialisation du modèle\n",
        "dimension_entree = 10  # Chaque entrée est un vecteur de taille 10\n",
        "dimension_sortie = 1   # Chaque cible est un scalaire\n",
        "modele = DeepMLP(dimension_entree, dimension_sortie).to(device)  # Déplacement du modèle vers le GPU\n",
        "\n",
        "# Définition de la fonction de perte et de l'optimiseur\n",
        "fonction_perte = nn.MSELoss()  # Erreur quadratique moyenne pour la régression\n",
        "optimiseur = optim.Adam(modele.parameters(), lr=0.001)  # Optimiseur Adam\n",
        "\n",
        "# Boucle d'entraînement\n",
        "nombre_epochs = 500\n",
        "for epoch in range(nombre_epochs):\n",
        "    for lot in chargeur_donnees:\n",
        "        x_lot, y_lot, _, indices_lot = lot\n",
        "\n",
        "        # Déplacement des données vers le GPU\n",
        "        x_lot = x_lot.to(device)\n",
        "        y_lot = y_lot.to(device)\n",
        "        indices_lot = indices_lot.to(device)\n",
        "\n",
        "        # Remise à zéro des gradients\n",
        "        optimiseur.zero_grad()\n",
        "\n",
        "        # Passe avant\n",
        "        predictions, theta_lot = modele(x_lot, indices_lot)\n",
        "\n",
        "        # Calcul de la perte\n",
        "        perte = fonction_perte(predictions, y_lot)\n",
        "\n",
        "        # Passe arrière et optimisation\n",
        "        perte.backward()\n",
        "        optimiseur.step()\n",
        "\n",
        "    # Affichage de la perte toutes les 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{nombre_epochs}], Perte : {perte.item():.4f}\")\n",
        "\n",
        "# Après l'entraînement, inspection des vecteurs theta appris\n",
        "print(\"Vecteurs theta appris :\")\n",
        "print(modele.theta)"
      ],
      "metadata": {
        "id": "UVmpuK5mnXY5",
        "outputId": "3339f43d-0a15-43b9-bf6f-e93e1f2356a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UVmpuK5mnXY5",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device utilisé : cuda\n",
            "Epoch [1/500], Perte : 210.4705\n",
            "Epoch [101/500], Perte : 1.0804\n",
            "Epoch [201/500], Perte : 0.5809\n",
            "Epoch [301/500], Perte : 0.2401\n",
            "Epoch [401/500], Perte : 0.1871\n",
            "Vecteurs theta appris :\n",
            "Parameter containing:\n",
            "tensor([[ 5.3656,  8.0830],\n",
            "        [ 0.0845, -1.7936],\n",
            "        [-7.8884,  5.8267],\n",
            "        [ 7.7862, -8.4657],\n",
            "        [-9.0142, -7.3800],\n",
            "        [ 5.7689,  7.6464],\n",
            "        [-0.0484, -1.6307],\n",
            "        [-7.4704,  5.9410],\n",
            "        [ 7.9559, -7.4387],\n",
            "        [-9.2191, -7.8458],\n",
            "        [ 5.7627,  7.9818],\n",
            "        [-0.0897, -1.6833],\n",
            "        [-7.6012,  5.9153],\n",
            "        [ 7.9696, -8.1819],\n",
            "        [-9.1198, -7.5318],\n",
            "        [ 5.5893,  7.6195],\n",
            "        [-0.0953, -1.6432],\n",
            "        [-7.6828,  5.7971],\n",
            "        [ 8.0401, -7.9132],\n",
            "        [-8.8164, -8.3767],\n",
            "        [ 5.4339,  8.0350],\n",
            "        [ 0.1147, -1.4896],\n",
            "        [-7.6254,  5.8859],\n",
            "        [ 8.2079, -7.1375],\n",
            "        [-8.1892, -7.8631],\n",
            "        [ 5.5997,  7.8303],\n",
            "        [-0.0811, -1.6699],\n",
            "        [-7.3679,  5.7121],\n",
            "        [ 7.9691, -7.2906],\n",
            "        [-9.0924, -7.5912],\n",
            "        [ 5.7181,  7.9149],\n",
            "        [-0.0126, -1.6651],\n",
            "        [-7.7817,  5.8135],\n",
            "        [ 7.6391, -7.6407],\n",
            "        [-9.0033, -7.9478],\n",
            "        [ 5.7008,  7.6275],\n",
            "        [-0.1398, -1.6200],\n",
            "        [-7.8309,  5.6175],\n",
            "        [ 8.2954, -7.1606],\n",
            "        [-8.5790, -7.8998],\n",
            "        [ 5.6501,  8.1041],\n",
            "        [-0.0115, -1.5772],\n",
            "        [-7.5348,  5.7851],\n",
            "        [ 7.5682, -8.8785],\n",
            "        [-8.8066, -7.3134],\n",
            "        [ 5.7911,  8.2445],\n",
            "        [-0.0787, -1.5014],\n",
            "        [-7.7112,  5.7921],\n",
            "        [ 8.3934, -7.4127],\n",
            "        [-8.4616, -8.4548]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2) L'objectif est de regrouper les données qui ont été générées avec la même fonction $f_k$$f_k$. Pour ce faire, le modèle apprend un vecteur 2D unique ( self.theta[k] ) pour chaque fonction $f_k$ (k allant de 0 à 4).Durant l'entraînement, self.theta est ajusté de telle sorte que les points appartenant au même groupe (même fonction $f_k$) soient proches dans l'espace 2D défini par self.theta.Tu peux vérifier cela après l'entraînement en visualisant les 50 vecteurs 2D de self.theta (qui correspondent aux 50 groupes de 1000 points) : les vecteurs correspondant au même k devraient être regroupés.\n",
        "\n",
        "- 3)**\"self.theta\" :** C'est le cœur du système de regroupement. Il stocke 50 vecteurs 2D, un pour chaque groupe de 1000 points. Chaque vecteur représente un point dans un espace latent 2D. Le modèle apprend à positionner ces vecteurs de manière à ce que les points appartenant au même groupe (même fonction $f_k$$f_k$) aient des vecteurs theta proches. **\"noise\" :** Le bruit gaussien ajouté à theta_batch agit comme une régularisation. Il empêche le modèle de simplement mémoriser les positions de theta pour chaque point et le force à apprendre une représentation plus générale. Cela permet au modèle de mieux généraliser à de nouvelles données. **\"indices // 1000\"** : Cette opération permet de récupérer l'indice du groupe (0 à 49) auquel appartient un point donné. Le jeu de données est construit de telle sorte que les 1000 premières données appartiennent au groupe 0, les 1000 suivantes au groupe 1, etc. En divisant l'indice global d'une donnée par 1000, on obtient l'indice du groupe auquel elle appartient. Ce groupe est utilisé ensuite pour récupérer le vecteur theta correspondant."
      ],
      "metadata": {
        "id": "ZCpogtnoWdw6"
      },
      "id": "ZCpogtnoWdw6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}